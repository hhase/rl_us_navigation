---
version: 1

kind: group

framework: pytorch

tags: [lego_phantom]

build:
  image: pytorch/pytorch:1.2-cuda10.0-cudnn7-runtime
  build_steps:
    - pip install -r requirements.txt
    - pip install tensorboardX

environment:
  resources:
    cpu:
      requests: 3
      limits: 8
    memory:
      requests: 1024
      limits: 4098
    gpu:
      requests: 1
      limits: 1

declarations:
  run_name:           "Noisy_Nets"
  #Grid parameters
  steps_x:            5
  steps_y:            20
  goal_x_coord:       2
  goal_y_coord:       10
  margin_x:           5
  margin_y:           8

  #Environment parameters
  priority_exponent:  0.5
  priority_weight:    0.1
  reward_correct_nop:  1.0
  reward_false_nop:   -0.25
  reward_border:      -0.1
  reward_move_closer:  0.05
  reward_move_away:   -0.1

  #General training parameters
  batch_size_train:   16
  lr:                 0.001
  episodes:           2000
  max_time_steps:     50
  log_interval:       50

  #RL training parameters
  memory_size:        10000
  min_memory_size:    8000
  target_update:      20
  target_update_lr:   0.001
  eps_start:          1.0
  eps_end:            0.01
  eps_decay:          0.0001
  eps_decay_begin:    2500
  gamma:              0.999
  prioritized_replay: true
  soft_target_update: true
  dueling_dqn:        true
  noisy_network:      false

hptuning:
  matrix:
    reward:
      values: [-0.05, -0.01, 0.05]


run:
  cmd: python -u main.py --run_name={{ run_name }}\
                         --steps_x={{ steps_x }}\
                         --steps_y={{ steps_y }}\
                         --goal_x_coord={{ goal_x_coord }}\
                         --goal_y_coord={{ goal_y_coord }}\
                         --margin_x={{ margin_x }}\
                         --margin_y={{ margin_y }}\
                         --priority_exponent={{ priority_exponent }}\
                         --priority_weight={{ priority_weight }}\
                         --reward_correct_nop={{ reward_correct_nop }}\
                         --reward_false_nop={{ reward_false_nop }}\
                         --reward_border={{ reward }}\
                         --reward_move_closer={{ reward + 0.01 }}\
                         --reward_move_away={{ reward }}\
                         --batch_size_train={{ batch_size_train }}\
                         --episodes={{ episodes }}\
                         --lr={{ lr }}\
                         --max_time_steps={{ max_time_steps }}\
                         --log_interval={{ log_interval }}\
                         --memory_size={{ memory_size }}\
                         --min_memory_size={{ min_memory_size }}\
                         --target_update={{ target_update }}\
                         --target_update_lr={{ target_update_lr }}\
                         --eps_start={{ eps_start }}\
                         --eps_end={{ eps_end }}\
                         --eps_decay={{ eps_decay }}\
                         --eps_decay_begin={{ eps_decay_begin }}\
                         --gamma={{ gamma }}\
                         --prioritized_replay={{ prioritized_replay }}\
                         --soft_target_update={{ soft_target_update }}\
                         --dueling_dqn={{ dueling_dqn }}\
                         --noisy_network={{ noisy_network }}
